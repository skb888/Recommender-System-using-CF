{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Read Data From CSV file\n",
    "import pandas as pd\n",
    "df=pd.read_csv('NEWDATATABLE.csv', sep=',',header=None)\n",
    "ratings = df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_test_generation(ratings):\n",
    "    #Cretae train and test matrix\n",
    "    test = np.zeros(ratings.shape)\n",
    "    train = ratings.copy()\n",
    "    for user in xrange(ratings.shape[0]):\n",
    "        temp = list(ratings[user, :])\n",
    "        temp_list = []\n",
    "        count = 0\n",
    "        # Choose top 10 known to be in test case\n",
    "        for item in temp:\n",
    "            if item != 99:\n",
    "                temp_list.append(item)\n",
    "        temp_list = sorted(temp_list, reverse=True)  \n",
    "        while count < 10:\n",
    "            count = count + 1\n",
    "            Index_temp = temp.index(temp_list[count])\n",
    "            #Set chosen be zero in training\n",
    "            train [user, Index_temp] = 0.\n",
    "            #Assign chosen rating to test case\n",
    "            test[user, Index_temp] = ratings[user, Index_temp]\n",
    "            ratings[user, Index_temp] = 0.\n",
    "            temp = list(ratings[user, :])\n",
    "            temp_list = []\n",
    "            for item in temp:\n",
    "                if item != 99:\n",
    "                    temp_list.append(item)\n",
    "            temp_list = sorted(temp_list, reverse=True)  \n",
    "              \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Call the function and then assign train and test\n",
    "train, test = train_test_generation(ratings)\n",
    "trainMAE = train.copy()\n",
    "#Masked for CF and not masked for SVD\n",
    "train = np.ma.masked_where(train == 99, train)\n",
    "test = np.ma.masked_where(test == 99, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Find averge of each column\n",
    "train_average_column = np.zeros(100)\n",
    "for indexI in range(100):\n",
    "    tempsum = 0;\n",
    "    count = 0; \n",
    "    for indexJ in range(24983):\n",
    "        if train[indexJ][indexI] != 99:\n",
    "            tempsum = tempsum + train[indexJ][indexI]\n",
    "            count = count + 1\n",
    "    train_average_column[indexI] = float(tempsum) / count\n",
    "#Find averge of each row\n",
    "train_average_row = np.zeros(24983)\n",
    "for indexI in range(24983):\n",
    "    tempsum = 0;\n",
    "    count = 0; \n",
    "    for indexJ in range(100):\n",
    "        if train[indexI][indexJ] != 99:\n",
    "            tempsum = tempsum + train[indexI][indexJ]\n",
    "            count = count + 1\n",
    "    train_average_row[indexI] = float(tempsum) / count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train_norm is used for CF item/user based here\n",
    "train_norm = train.copy()\n",
    "#Fill the sparse with avergae of item score\n",
    "for indexI in range(24983):\n",
    "    for indexJ in range(100):\n",
    "        if train_norm[indexI][indexJ] == 99:\n",
    "            train_norm[indexI][indexJ] = train_average_column[indexJ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Fcn to compute user or item similarity\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def similarity(ratings, kind='user'):\n",
    "    if kind == 'user':\n",
    "        sim = cosine_similarity(ratings)\n",
    "        assert(sim.shape[0] == ratings.shape[0])\n",
    "    elif kind == 'item':\n",
    "        sim = cosine_similarity(ratings.T)\n",
    "        assert(sim.shape[0] == ratings.shape[1])\n",
    "    sim[np.isnan(sim)] = 0\n",
    "    np.fill_diagonal(sim,0)\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Compute item and user similarity\n",
    "item_similarity = similarity(train, kind='item')\n",
    "user_similarity = similarity(train, kind='user')\n",
    "#Copy them for finding index in KNN\n",
    "item_similarity_fcn = item_similarity.copy()\n",
    "user_similarity_fcn = user_similarity.copy()\n",
    "#Copy them for computing Top K CF based\n",
    "item_similarity_CF = item_similarity.copy()\n",
    "user_similarity_CF = user_similarity.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(ratings, similarity, kind ='user'):\n",
    "    if kind == 'user':\n",
    "        user_bias = ratings.mean(axis = 1)\n",
    "        ratings = (ratings - user_bias[:, np.newaxis]).copy()\n",
    "        pred = np.ma.dot(similarity, ratings) / np.array([np.abs(similarity).sum(axis=1)+1e-6]).T\n",
    "        pred += user_bias[:, np.newaxis]\n",
    "    elif kind == 'item':\n",
    "        item_bias = ratings.mean(axis = 0)\n",
    "        ratings = (ratings - item_bias[np.newaxis, :]).copy()\n",
    "        pred = np.ma.dot(ratings, similarity) / np.array([np.abs(similarity).sum(axis=1)+1e-6])\n",
    "        pred += item_bias[np.newaxis, :]\n",
    "        \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_prediction = predict(train, user_similarity, kind = 'user')\n",
    "item_prediction = predict(train, item_similarity, kind = 'item')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Assign user_prediction or item_prediction to parameter\n",
    "all_user_predicted_ratings = user_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 30 items for each user in test case and choose Top-N\n",
    "#Pick unkonwn index\n",
    "#evaluate recall and precise\n",
    "unknown_index = []\n",
    "exist_row = []\n",
    "#discard\n",
    "notexist_row = []\n",
    "for indexI in range(24983):\n",
    "    temp_row_index = np.where(trainMAE[indexI] == 99)\n",
    "    if len(list(temp_row_index[0])) < 30 :\n",
    "            notexist_row.append(indexI)\n",
    "    else:\n",
    "        temp_row_index_final = list(np.random.choice(temp_row_index[0], size = 30, replace = False))\n",
    "        unknown_index.append(temp_row_index_final)\n",
    "        exist_row.append(indexI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Find unknown rating for each user in training\n",
    "from collections import defaultdict\n",
    "combine = zip(exist_row, unknown_index)\n",
    "row_number_result = defaultdict(list)\n",
    "for indexI in range(len(combine)):\n",
    "    row_temp = list()\n",
    "    for item in combine[indexI][1]:\n",
    "        row_temp.append(all_user_predicted_ratings[combine[indexI][0]][item])\n",
    "    row_number_result[combine[indexI][0]].append(row_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Find known rating for each user in test\n",
    "row_test_result = defaultdict(list)\n",
    "for indexI in range(len(combine)):\n",
    "    a = test[combine[indexI][0]]\n",
    "    a = a[a != 0]\n",
    "    a = list(a)\n",
    "    row_test_result[combine[indexI][0]].append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def recall_precise(index_matrix, test_index):\n",
    "    recall_result = []\n",
    "    precise_result = []\n",
    "    N = 15\n",
    "    recall_temp = 0\n",
    "    precise_temp = 0\n",
    "    \n",
    "    for item in exist_row:\n",
    "        temp_train_list = index_matrix[item][0]\n",
    "        for item_test in row_test_result[item][0]:\n",
    "            temp_train_list.append(item_test)\n",
    "            temp_sort_list = sorted(temp_train_list, reverse = True)\n",
    "            temp_index = temp_sort_list.index(item_test)\n",
    "            if temp_index < N:\n",
    "                count = 1\n",
    "            else:\n",
    "                count = 0 \n",
    "            recall_temp = count\n",
    "            precise_temp = float(count) / N\n",
    "            recall_result.append(recall_temp)\n",
    "            precise_result.append(precise_temp)\n",
    "    return recall_result, precise_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Call two fcns to achieve to find\n",
    "recall_each, precise_each = recall_precise(row_number_result, row_test_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.997260664569\n",
      "Precise: 0.0664840443045\n"
     ]
    }
   ],
   "source": [
    "recall_AVG = float(sum(recall_each)) / len(recall_each)\n",
    "precise_AVG = float(sum(precise_each)) / len(precise_each)\n",
    "print 'Recall: ' + str(recall_AVG)\n",
    "print 'Precise: ' + str(precise_AVG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10337"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(row_number_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
